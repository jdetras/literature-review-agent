# AI Agent Workflow Architecture
## Genomic LLM Literature Review System

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER INTERFACE LAYER                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ CLI Script   â”‚  â”‚ Python API   â”‚  â”‚ Config File  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI AGENT CORE LAYER                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚            GenomicLLMAgent (Main Controller)              â”‚ â”‚
â”‚  â”‚  â€¢ Query Planning                                         â”‚ â”‚
â”‚  â”‚  â€¢ Source Selection                                       â”‚ â”‚
â”‚  â”‚  â€¢ Result Aggregation                                     â”‚ â”‚
â”‚  â”‚  â€¢ Gap Analysis                                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA ACQUISITION LAYER                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  bioRxiv    â”‚  â”‚    PMC      â”‚  â”‚   arXiv     â”‚            â”‚
â”‚  â”‚  Scraper    â”‚  â”‚  E-Utils    â”‚  â”‚    API      â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚         â”‚                 â”‚                 â”‚                    â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                           â”‚                                      â”‚
â”‚                           â–¼                                      â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                  â”‚  Rate Limiter   â”‚                            â”‚
â”‚                  â”‚  Error Handler  â”‚                            â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 PROCESSING & ANALYSIS LAYER                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  Publication Parser  â”‚  â”‚   Deduplicator       â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                â”‚                      â”‚                          â”‚
â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                           â–¼                                      â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚            â”‚   [Optional] Claude AI      â”‚                      â”‚
â”‚            â”‚   â€¢ Extract model names     â”‚                      â”‚
â”‚            â”‚   â€¢ Identify crops          â”‚                      â”‚
â”‚            â”‚   â€¢ Summarize findings      â”‚                      â”‚
â”‚            â”‚   â€¢ List datasets           â”‚                      â”‚
â”‚            â”‚   â€¢ Note limitations        â”‚                      â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                           â”‚                                      â”‚
â”‚                           â–¼                                      â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚            â”‚   Gap Analyzer              â”‚                      â”‚
â”‚            â”‚   â€¢ Coverage analysis       â”‚                      â”‚
â”‚            â”‚   â€¢ Trend detection         â”‚                      â”‚
â”‚            â”‚   â€¢ Opportunity mapping     â”‚                      â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     OUTPUT GENERATION LAYER                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  JSON Report   â”‚  â”‚  Text Summary  â”‚  â”‚  CSV Export    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Workflow Sequence

### Phase 1: Initialization
```
1. Load Configuration
   â”œâ”€â†’ Read target models list
   â”œâ”€â†’ Read cereal crops list
   â”œâ”€â†’ Load search queries
   â””â”€â†’ Configure data sources

2. Initialize API Clients
   â”œâ”€â†’ Setup HTTP session with headers
   â”œâ”€â†’ Configure rate limiting
   â””â”€â†’ [Optional] Initialize Claude API
```

### Phase 2: Literature Search
```
For each search query:
  â”‚
  â”œâ”€â†’ bioRxiv Search
  â”‚   â”œâ”€â†’ Construct search URL
  â”‚   â”œâ”€â†’ Send HTTP request
  â”‚   â”œâ”€â†’ Parse HTML response
  â”‚   â”œâ”€â†’ Extract article metadata
  â”‚   â””â”€â†’ Rate limit delay (1-2s)
  â”‚
  â”œâ”€â†’ PubMed Central Search
  â”‚   â”œâ”€â†’ Query esearch API
  â”‚   â”œâ”€â†’ Get PMC IDs
  â”‚   â”œâ”€â†’ Fetch article summaries
  â”‚   â”œâ”€â†’ Extract metadata
  â”‚   â””â”€â†’ Rate limit delay
  â”‚
  â””â”€â†’ arXiv Search
      â”œâ”€â†’ Query arXiv API
      â”œâ”€â†’ Parse XML response
      â”œâ”€â†’ Extract paper details
      â””â”€â†’ Rate limit delay
```

### Phase 3: Data Processing
```
1. Deduplication
   â”œâ”€â†’ Create title-based hash
   â”œâ”€â†’ Compare with existing entries
   â””â”€â†’ Keep unique publications

2. [Optional] AI Enhancement
   For each publication:
   â”œâ”€â†’ Prepare analysis prompt
   â”œâ”€â†’ Call Claude API
   â”œâ”€â†’ Parse JSON response
   â”œâ”€â†’ Update publication object
   â””â”€â†’ Rate limit delay

3. Validation
   â”œâ”€â†’ Check required fields
   â”œâ”€â†’ Validate URLs
   â””â”€â†’ Flag incomplete records
```

### Phase 4: Gap Analysis
```
1. Coverage Analysis
   â”œâ”€â†’ Count publications per model
   â”œâ”€â†’ Count publications per crop
   â””â”€â†’ Identify underrepresented areas

2. Trend Detection
   â”œâ”€â†’ Analyze publication years
   â”œâ”€â†’ Identify emerging models
   â””â”€â†’ Track research momentum

3. Gap Identification
   â”œâ”€â†’ Compare vs. threshold (3 pubs)
   â”œâ”€â†’ Identify missing applications
   â”œâ”€â†’ List dataset gaps
   â”œâ”€â†’ Note methodological gaps
   â””â”€â†’ Highlight rice-specific opportunities
```

### Phase 5: Report Generation
```
1. Compile Data
   â”œâ”€â†’ Aggregate all publications
   â”œâ”€â†’ Structure gap analysis
   â””â”€â†’ Generate recommendations

2. Create JSON Report
   â”œâ”€â†’ Format metadata
   â”œâ”€â†’ Serialize publications
   â”œâ”€â†’ Include gap analysis
   â””â”€â†’ Write to file

3. Create Text Summary
   â”œâ”€â†’ Generate human-readable overview
   â”œâ”€â†’ Format statistics
   â”œâ”€â†’ List key findings
   â””â”€â†’ Write to file
```

---

## ğŸ¯ Component Details

### GenomicLLMAgent Class

**Responsibilities:**
- Orchestrate entire workflow
- Manage data sources
- Coordinate searches
- Perform gap analysis
- Generate reports

**Key Methods:**
```python
search_biorxiv(query, max_results)
search_pubmed_central(query, max_results)
search_arxiv(query, max_results)
analyze_publication_with_claude(publication)
run_literature_search(use_claude)
identify_research_gaps()
generate_literature_review_report(output_file)
```

### Publication Data Structure

```python
@dataclass
class Publication:
    title: str              # Paper title
    authors: List[str]      # Author list
    year: int              # Publication year
    abstract: str          # Full abstract
    url: str               # Direct link
    source: str            # Data source (bioRxiv/PMC/arXiv)
    model_name: str        # Genomic LLM discussed
    crop_focus: List[str]  # Crops mentioned
    key_findings: List[str] # Main discoveries
    methodology: str       # Experimental approach
    datasets_used: List[str] # Data resources
    limitations: List[str]  # Stated gaps
```

### Search Strategy

**Multi-Source Approach:**
1. **bioRxiv**: Biology preprints, latest research
2. **PMC**: Peer-reviewed open access papers
3. **arXiv**: CS/ML papers on genomic models

**Query Construction:**
- Combine model names with crop terms
- Include synonyms (e.g., "corn" and "maize")
- Add domain keywords ("genomic", "transformer")

**Pagination:**
- Request 10-20 results per query
- Multiple queries cover broader space
- Deduplication ensures uniqueness

---

## ğŸ”¬ AI Analysis Pipeline (Optional)

### When Claude is Enabled:

```
For each publication:
  â”‚
  â”œâ”€â†’ Prepare Structured Prompt
  â”‚   â”œâ”€â†’ Include title & abstract
  â”‚   â”œâ”€â†’ Specify extraction format (JSON)
  â”‚   â””â”€â†’ Define target fields
  â”‚
  â”œâ”€â†’ Call Anthropic API
  â”‚   â”œâ”€â†’ Model: claude-sonnet-4
  â”‚   â”œâ”€â†’ Max tokens: 1000
  â”‚   â””â”€â†’ Temperature: 0.3 (for consistency)
  â”‚
  â”œâ”€â†’ Parse Response
  â”‚   â”œâ”€â†’ Extract JSON from text
  â”‚   â”œâ”€â†’ Validate field types
  â”‚   â””â”€â†’ Handle parsing errors
  â”‚
  â””â”€â†’ Update Publication
      â”œâ”€â†’ Set model_name
      â”œâ”€â†’ Set crop_focus
      â”œâ”€â†’ Set key_findings
      â”œâ”€â†’ Set methodology
      â”œâ”€â†’ Set datasets_used
      â””â”€â†’ Set limitations
```

### Example Claude Prompt:

```
Analyze this genomic LLM research paper:

Title: [TITLE]
Abstract: [ABSTRACT]

Extract:
1. Genomic LLM model(s) discussed
2. Crop species focus (especially cereals)
3. Key findings (3-5 main points)
4. Methodology used
5. Datasets mentioned
6. Stated limitations

Return as JSON: {
  "model_name": "...",
  "crop_focus": [...],
  "key_findings": [...],
  "methodology": "...",
  "datasets_used": [...],
  "limitations": [...]
}
```

---

## ğŸ“Š Gap Analysis Algorithm

### Step 1: Count Coverage
```python
crop_mentions = defaultdict(int)
model_mentions = defaultdict(int)

for publication in publications:
    for crop in publication.crop_focus:
        crop_mentions[crop] += 1
    
    if publication.model_name:
        model_mentions[publication.model_name] += 1
```

### Step 2: Identify Gaps
```python
threshold = 3  # Minimum publications

for crop in target_crops:
    if crop_mentions[crop] < threshold:
        gaps["understudied_crops"].append(crop)
```

### Step 3: Detect Missing Applications
```python
# Check for absence of key application areas
applications = [
    "climate stress prediction",
    "yield optimization",
    "disease resistance",
    "pan-genome analysis",
    "breeding integration"
]

for app in applications:
    app_count = count_application_mentions(publications, app)
    if app_count < threshold:
        gaps["missing_applications"].append(app)
```

### Step 4: Rice-Specific Analysis
```python
rice_pub_count = crop_mentions.get("rice", 0)
rice_models = models_applied_to_rice(publications)

if rice_pub_count < 5:
    gaps["rice_specific_gaps"].append(
        "Limited genomic LLM research specifically for rice"
    )

if len(rice_models) < 3:
    gaps["rice_specific_gaps"].append(
        "Few models fine-tuned on rice genomic data"
    )
```

---

## âš™ï¸ Configuration Options

### Rate Limiting
```json
{
  "rate_limiting": {
    "requests_per_minute": 30,
    "delay_between_requests": 2.0,
    "retry_attempts": 3,
    "retry_delay": 5.0
  }
}
```

### Search Filters
```json
{
  "search_filters": {
    "min_year": 2020,
    "max_year": 2026,
    "languages": ["en"],
    "open_access_only": true
  }
}
```

### Output Settings
```json
{
  "output_settings": {
    "output_directory": "/mnt/user-data/outputs",
    "json_filename": "literature_review.json",
    "summary_filename": "literature_review_summary.txt",
    "include_abstracts": true,
    "pretty_print": true
  }
}
```

---

## ğŸ” Error Handling

### Network Errors
```python
try:
    response = requests.get(url, timeout=30)
    response.raise_for_status()
except requests.RequestException as e:
    logger.error(f"Network error: {e}")
    retry_with_exponential_backoff()
```

### Parsing Errors
```python
try:
    soup = BeautifulSoup(content, 'html.parser')
    title = soup.find('title').text
except AttributeError:
    logger.warning(f"Could not parse title from {url}")
    continue  # Skip to next result
```

### API Errors
```python
try:
    response = anthropic_client.messages.create(...)
except APIError as e:
    logger.error(f"Claude API error: {e}")
    # Continue without AI analysis
```

---

## ğŸ“ˆ Performance Considerations

### Speed Optimization
- **Parallel Requests**: Can implement concurrent searches
- **Caching**: Store results to avoid re-scraping
- **Batch Processing**: Group API calls

### Resource Usage
- **Memory**: ~100MB for 500 publications
- **Network**: 1-5 MB per search query
- **Time**: 
  - Without Claude: ~10 minutes for 100 publications
  - With Claude: ~30 minutes for 100 publications

### Scalability
- **Publications**: Tested up to 1000 publications
- **Queries**: Can handle 50+ search queries
- **Sources**: Extensible to additional databases

---

## ğŸš€ Future Enhancements

### Potential Improvements
1. **Full-Text Analysis**: Extract from PDF papers
2. **Citation Network**: Build citation graphs
3. **Collaboration Detection**: Identify research groups
4. **Trend Forecasting**: Predict emerging topics
5. **Automated Alerts**: Notify on new publications
6. **Interactive Dashboard**: Web UI for exploration
7. **Export Formats**: BibTeX, EndNote, Zotero

### Additional Data Sources
- **Google Scholar**: Broader coverage
- **Semantic Scholar**: Better metadata
- **Europe PMC**: European publications
- **DOAJ**: Directory of Open Access Journals

---

## ğŸ“š Integration Possibilities

### With Research Tools
```python
# Export to reference manager
def export_to_bibtex(publications):
    # Generate .bib file

# Link to Zotero
def sync_with_zotero(publications, api_key):
    # Upload to Zotero library
```

### With Analysis Tools
```python
# Network analysis
import networkx as nx
G = build_citation_network(publications)

# Topic modeling
from sklearn.decomposition import LatentDirichletAllocation
topics = lda.fit_transform(abstracts)
```

### With Visualization
```python
import plotly.express as px

# Timeline visualization
fig = px.timeline(publications, x_start="year", color="model_name")

# Geographic distribution
fig = px.scatter_geo(publications, locations="institution_country")
```

---

**End of Architecture Document**

This agent provides a comprehensive foundation for automated literature review in genomic AI research. The modular design allows for easy customization and extension based on specific research needs.
